{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "electra_classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNYQvQrQ6WNJCYc3Vem2VSv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AyushiM1102/Electra_classification_fake_vs_real_news/blob/main/electra_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference tutorial:\n",
        "# https://velog.io/@na2na8/ELECTRA%EB%A1%9C-Binary-Classification#electra-with-pytorch-lightning"
      ],
      "metadata": {
        "id": "0zfy0r0jjBdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install emoji --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4dAKUSJgiyq",
        "outputId": "87b73d29-07f9-41d3-e6f8-6d6315dae1bc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 40 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 51 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 61 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 81 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 92 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 102 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 112 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 122 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 133 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 143 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 153 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 163 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 174 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 175 kB 5.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=3d6330685e66d159212e6f33c312c504a75a1136026d3a9333cb7e344bc16525\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/PyTorchLightning/pytorch-lightning --quiet\n",
        "import pytorch_lightning as pl\n",
        "print(pl.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9m_Cni2Xg07r",
        "outputId": "98ec3a2c-60fe-41ca-add3-ae19f1e98616"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "1.7.0dev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "#import emoji\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#from soynlp.normalizer import repeat_normalize\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "#import pytorch_lightning as pl\n",
        "from pytorch_lightning import loggers as pl_loggers\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "import transformers\n",
        "from transformers import ElectraForSequenceClassification, ElectraTokenizer, AdamW"
      ],
      "metadata": {
        "id": "XgSslOGfgaBW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ElectraClassificationDataset(Dataset) :\n",
        "    def __init__(self, path, sep, doc_col, label_col, max_length, \n",
        "                num_workers=1, labels_dict=None) :\n",
        "        self.tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-small-v3-discriminator\")\n",
        "\n",
        "        self.max_length = max_length\n",
        "        self.doc_col = doc_col\n",
        "        self.label_col = label_col\n",
        "\n",
        "        # labels\n",
        "        # None : label이 num으로 되어 있음\n",
        "        # dict : label이 num이 아닌 것으로 되어 있음\n",
        "        # ex : {True : 1, False : 0}\n",
        "        self.labels_dict = labels_dict\n",
        "\n",
        "        # dataset\n",
        "        df = pd.read_csv(path, sep=sep)\n",
        "        df = df.dropna(axis=0)\n",
        "        df.drop_duplicates(subset=[self.doc_col], inplace=True)\n",
        "        self.dataset = df\n",
        "\n",
        "    def __len__(self) :\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def cleanse(self, text) :\n",
        "        emojis = ''.join(emoji.UNICODE_EMOJI.keys())  \n",
        "        pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-힣{emojis}]+')\n",
        "        url_pattern = re.compile(\n",
        "            r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)'\n",
        "        )\n",
        "        processed = pattern.sub(' ', text)\n",
        "        processed = url_pattern.sub(' ', processed)\n",
        "        processed = processed.strip()\n",
        "        processed = repeat_normalize(processed, num_repeats=2)\n",
        "      \n",
        "        return processed\n",
        "\n",
        "    def __getitem__(self, idx) :\n",
        "        document = self.cleanse(self.dataset[self.doc_col].iloc[idx])\n",
        "        inputs = self.tokenizer(\n",
        "            document,\n",
        "            return_tensors='pt',\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            add_special_tokens=True\n",
        "        )\n",
        "\n",
        "        if self.labels_dict :\n",
        "            label = self.labels_dict[self.dataset[self.label_col].iloc[idx]]\n",
        "        else :\n",
        "            label = self.dataset[self.label_col].iloc[idx]\n",
        "\n",
        "        return {\n",
        "            'input_ids' : inputs['input_ids'][0],\n",
        "            'attention_mask' : inputs['attention_mask'][0],\n",
        "            'label' : int(label)\n",
        "        }"
      ],
      "metadata": {
        "id": "qG2NiXYBhWub"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ElectraClassificationDataModule(pl.LightningDataModule) :\n",
        "    def __init__(self, train_path, valid_path, max_length, batch_size, sep,\n",
        "                doc_col, label_col, num_workers=1, labels_dict=None) :\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.train_path = train_path\n",
        "        self.valid_path = valid_path\n",
        "        self.max_length = max_length\n",
        "        self.doc_col = doc_col\n",
        "        self.label_col = label_col\n",
        "        self.sep = sep\n",
        "        self.num_workers = num_workers\n",
        "        self.labels_dict = labels_dict\n",
        "\n",
        "    def setup(self, stage=None) :\n",
        "        self.set_train = ElectraClassificationDataset(self.train_path, sep=self.sep,\n",
        "                                            doc_col=self.doc_col, label_col=self.label_col,\n",
        "                                            max_length = self.max_length, labels_dict=self.labels_dict)\n",
        "        self.set_valid = ElectraClassificationDataset(self.valid_path, sep=self.sep,\n",
        "                                            doc_col=self.doc_col, label_col=self.label_col,\n",
        "                                            max_length = self.max_length, labels_dict=self.labels_dict)\n",
        "\n",
        "    def train_dataloader(self) :\n",
        "        train = DataLoader(self.set_train, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n",
        "        return train\n",
        "    \n",
        "    def val_dataloader(self) :\n",
        "        val = DataLoader(self.set_valid, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False)\n",
        "        return val\n",
        "    \n",
        "    def test_dataloader(self) :\n",
        "        test = DataLoader(self.set_valid, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False)\n",
        "        return test"
      ],
      "metadata": {
        "id": "dvv9q3-phfSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "electra = ElectraForSequenceClassification.from_pretrained(\"monologg/koelectra-small-v3-discriminator\")\n",
        "\n",
        "dm = ElectraClassificationDataModule(batch_size=8, train_path='./ratings_train_pre.txt', valid_path='./ratings_test_pre.txt',\n",
        "                                    max_length=256, sep='\\t', doc_col='document', label_col='label', num_workers=1)\n",
        "\n",
        "dm.setup()\n",
        "\n",
        "t = dm.train_dataloader()\n",
        "\n",
        "for idx, data in enumerate(t) :\n",
        "    print(idx, data['input_ids'].shape, data['attention_mask'].shape, data['label'].shape)\n",
        "\n",
        "v = dm.val_dataloader()\n",
        "for idx, data in enumerate(v) :\n",
        "    print(idx, data['input_ids'].shape, data['attention_mask'].shape, data['label'].shape)\n",
        "idx, data = enumerate(t)\n",
        "\n",
        "print(data['input_ids'])\n",
        "print(data['input_ids'].shape)\n",
        "\n",
        "print(data['attention_mask'])\n",
        "print(data['attention_mask'].shape)\n",
        "\n",
        "print(data['label'])\n",
        "print(data['label'].shape)\n",
        "\n",
        "output = electra.forward(data['input_ids'], attention_mask=data['attention_mask'], labels=data['label'].view([-1,1]))\n",
        "\n",
        "print(output.loss)\n",
        "# print(output.loss.shape)\n",
        "print(output.logits)\n",
        "print(output.logits.shape)\n",
        "\n",
        "softmax = nn.functional.softmax(output.logits, dim=1)\n",
        "print('softmax', softmax)\n",
        "pred = softmax.argmax(dim=1)\n",
        "print('pred', pred)\n",
        "\n",
        "y_true = data['label'].tolist()\n",
        "y_pred = pred.tolist()\n",
        "\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "prec = precision_score(y_true, y_pred)\n",
        "rec = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(f'acc : {acc}, prec : {prec}, rec : {rec}, f1 : {f1}')\n"
      ],
      "metadata": {
        "id": "hFQuspRvhpX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torchmetrics\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import loggers as pl_loggers\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "import transformers\n",
        "from transformers import ElectraForSequenceClassification, ElectraTokenizer, AdamW\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "# https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d\n",
        "# https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/electra#transformers.ElectraForSequenceClassification"
      ],
      "metadata": {
        "id": "c1iIOU45h1vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ElectraClassification(pl.LightningModule) :\n",
        "    def __init__(self, learning_rate) :\n",
        "        super().__init__()\n",
        "        self.learning_rate = learning_rate\n",
        "        self.save_hyperparameters()\n",
        "        \n",
        "        self.electra = ElectraForSequenceClassification.from_pretrained(\"monologg/koelectra-small-v3-discriminator\")\n",
        "\n",
        "        self.metric_acc = torchmetrics.Accuracy()\n",
        "        self.metric_f1 = torchmetrics.F1(num_classes=2)\n",
        "        self.metric_rec = torchmetrics.Recall(num_classes=2)\n",
        "        self.metric_pre = torchmetrics.Precision(num_classes=2)\n",
        "\n",
        "        self.loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None) :\n",
        "        output = self.electra(input_ids=input_ids, \n",
        "                                attention_mask=attention_mask, \n",
        "                                labels=labels)\n",
        "        return output\n",
        "\n",
        "    def training_step(self, batch, batch_idx) :\n",
        "        '''\n",
        "        ##########################################################\n",
        "        electra forward input shape information\n",
        "        * input_ids.shape (batch_size, max_length)\n",
        "        * attention_mask.shape (batch_size, max_length)\n",
        "        * label.shape (batch_size,)\n",
        "        ##########################################################\n",
        "        '''\n",
        "\n",
        "        # change label shape (list -> torch.Tensor((batch_size, 1)))\n",
        "        label = batch['label'].view([-1,1])\n",
        "\n",
        "        output = self(input_ids=batch['input_ids'].to(device),\n",
        "                        attention_mask=batch['attention_mask'].to(device),\n",
        "                        labels=label.to(device))\n",
        "        '''\n",
        "        ##########################################################\n",
        "        electra forward output shape information\n",
        "        * loss.shape (1,)\n",
        "        * logits.shape (batch_size, config.num_labels=2)\n",
        "        '''\n",
        "        logits = output.logits\n",
        "\n",
        "        loss = output.loss\n",
        "        # loss = self.loss_func(logits.to(device), batch['label'].to(device))\n",
        "\n",
        "        softmax = nn.functional.softmax(logits, dim=1)\n",
        "        preds = softmax.argmax(dim=1)\n",
        "\n",
        "        self.log(\"train_loss\", loss, prog_bar=True)\n",
        "        \n",
        "        return {\n",
        "            'loss' : loss,\n",
        "            'pred' : preds,\n",
        "            'label' : batch['label']\n",
        "        }\n",
        "\n",
        "    def training_epoch_end(self, outputs, state='train') :\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        for i in outputs :\n",
        "            y_true += i['label'].tolist()\n",
        "            y_pred += i['pred'].tolist()\n",
        "\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        prec = precision_score(y_true, y_pred)\n",
        "        rec = recall_score(y_true, y_pred)\n",
        "        f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "        # self.log(state+'_acc', acc, on_epoch=True, prog_bar=True)\n",
        "        # self.log(state+'_precision', prec, on_epoch=True, prog_bar=True)\n",
        "        # self.log(state+'_recall', rec, on_epoch=True, prog_bar=True)\n",
        "        # self.log(state+'_f1', f1, on_epoch=True, prog_bar=True)\n",
        "        print(f'[Epoch {self.trainer.current_epoch} {state.upper()}] Acc: {acc}, Prec: {prec}, Rec: {rec}, F1: {f1}')\n",
        "\n",
        "    def validation_step(self, batch, batch_idx) :\n",
        "        '''\n",
        "        ##########################################################\n",
        "        electra forward input shape information\n",
        "        * input_ids.shape (batch_size, max_length)\n",
        "        * attention_mask.shape (batch_size, max_length)\n",
        "        ##########################################################\n",
        "        '''\n",
        "        output = self(input_ids=batch['input_ids'].to(device),\n",
        "                        attention_mask=batch['attention_mask'].to(device))\n",
        "        logits = output.logits\n",
        "        preds = nn.functional.softmax(logits, dim=1).argmax(dim=1)\n",
        "\n",
        "        labels = batch['label']\n",
        "        accuracy = self.metric_acc(preds, labels)\n",
        "        f1 = self.metric_f1(preds, labels)\n",
        "        recall = self.metric_rec(preds, labels)\n",
        "        precision = self.metric_pre(preds, labels)\n",
        "        self.log('val_accuracy', accuracy, on_epoch=True, prog_bar=True)\n",
        "        self.log('val_f1', f1, on_epoch=True, prog_bar=True)\n",
        "        self.log('val_recall', recall, on_epoch=True, prog_bar=True)\n",
        "        self.log('val_precision', precision, on_epoch=True, prog_bar=True)\n",
        "\n",
        "        return {\n",
        "            'accuracy' : accuracy,\n",
        "            'f1' : f1,\n",
        "            'recall' : recall,\n",
        "            'precision' : precision\n",
        "        }\n",
        "\n",
        "    def validation_epoch_end(self, outputs) :\n",
        "        val_acc = torch.stack([i['accuracy'] for i in outputs]).mean()\n",
        "        val_f1 = torch.stack([i['f1'] for i in outputs]).mean()\n",
        "        val_rec = torch.stack([i['recall'] for i in outputs]).mean()\n",
        "        val_pre = torch.stack([i['precision'] for i in outputs]).mean()\n",
        "        # self.log('val_f1', val_f1, on_epoch=True, prog_bar=True)\n",
        "        # self.log('val_acc', val_acc, on_epoch=True, prog_bar=True)\n",
        "        print(f'val_accuracy : {val_acc}, val_f1 : {val_f1}, val_recall : {val_rec}, val_precision : {val_pre}')\n",
        "        \n",
        "    \n",
        "    def configure_optimizers(self) :\n",
        "        optimizer = torch.optim.AdamW(self.electra.parameters(), lr=self.learning_rate)\n",
        "        lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "        \n",
        "        return {\n",
        "            'optimizer' : optimizer,\n",
        "            'lr_scheduler' : lr_scheduler\n",
        "        }"
      ],
      "metadata": {
        "id": "dr1NkLgFh-oR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    model = ElectraClassification(learning_rate=0.0001)\n",
        "\n",
        "    dm = ElectraClassificationDataModule(batch_size=8, train_path='./ratings_train_pre.txt', valid_path='./ratings_test_pre.txt',\n",
        "                                    max_length=256, sep='\\t', doc_col='document', label_col='label', num_workers=1)\n",
        "    \n",
        "    checkpoint_callback = pl.callbacks.ModelCheckpoint(monitor='val_accuracy',\n",
        "                                                    dirpath='./sample_electra_binary_nsmc_chpt',\n",
        "                                                    filename='KoELECTRA/{epoch:02d}-{val_accuracy:.3f}',\n",
        "                                                    verbose=True,\n",
        "                                                    save_last=True,\n",
        "                                                    mode='max',\n",
        "                                                    save_top_k=-1,\n",
        "                                                    )\n",
        "    \n",
        "    tb_logger = pl_loggers.TensorBoardLogger(os.path.join('./sample_electra_binary_nsmc_chpt', 'tb_logs'))\n",
        "\n",
        "    lr_logger = pl.callbacks.LearningRateMonitor()\n",
        "\n",
        "    trainer = pl.Trainer(\n",
        "        default_root_dir='./sample_electra_binary_nsmc_chpt/checkpoints',\n",
        "        logger = tb_logger,\n",
        "        callbacks = [checkpoint_callback, lr_logger],\n",
        "        max_epochs=3,\n",
        "        gpus=1\n",
        "    )\n",
        "\n",
        "    trainer.fit(model, dm)"
      ],
      "metadata": {
        "id": "a4e9tpVeiHXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def infer(x, path) :\n",
        "    model = ElectraClassification.load_from_checkpoint(path)\n",
        "    tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-small-v3-discriminator\")\n",
        "    \n",
        "    emojis = ''.join(emoji.UNICODE_EMOJI.keys())  \n",
        "    pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-힣{emojis}]+')\n",
        "    url_pattern = re.compile(\n",
        "        r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)'\n",
        "    )\n",
        "    processed = pattern.sub(' ', x)\n",
        "    processed = url_pattern.sub(' ', processed)\n",
        "    processed = processed.strip()\n",
        "    processed = repeat_normalize(processed, num_repeats=2)\n",
        "\n",
        "    tokenized = tokenizer(processed, return_tensors='pt')\n",
        "\n",
        "    output = model(tokenized.input_ids, tokenized.attention_mask)\n",
        "    return nn.functional.softmax(output.logits, dim=-1)"
      ],
      "metadata": {
        "id": "-aF6v8sEicsv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}